{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f57bf77",
   "metadata": {},
   "source": [
    "## 일반적인 NLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3ebc984",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"nlp\").getOrCreate() # 데이터프레임 형태로 읽어들임"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c456cf22",
   "metadata": {},
   "source": [
    "### Tokenzier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7484289c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer #데이터들을 쪼개는 작업들을 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40b001e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"NLP.txt\",header=False,inferSchema=True,sep=\"\\t\")# 텍스트파일을 읽어옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3731f9ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "| _c0|                 _c1|\n",
      "+----+--------------------+\n",
      "|spam|here is a list of...|\n",
      "| ham|Go until jurong p...|\n",
      "| ham|Ok lar... Joking ...|\n",
      "|spam|Free entry in 2 a...|\n",
      "| ham|U dun say so earl...|\n",
      "| ham|Nah I don't think...|\n",
      "|spam|FreeMsg Hey there...|\n",
      "| ham|Even my brother i...|\n",
      "| ham|As per your reque...|\n",
      "|spam|WINNER!! As a val...|\n",
      "|spam|Had your mobile 1...|\n",
      "| ham|I'm gonna be home...|\n",
      "|spam|SIX chances to wi...|\n",
      "|spam|URGENT! You have ...|\n",
      "| ham|I've been searchi...|\n",
      "| ham|I HAVE A DATE ON ...|\n",
      "|spam|XXXMobileMovieClu...|\n",
      "| ham|Oh k...i'm watchi...|\n",
      "| ham|Eh u remember how...|\n",
      "| ham|Fine if thats th...|\n",
      "+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4574aa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumnRenamed(\"_c0\",\"label\").withColumnRenamed(\"_c1\",\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8accde6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|                text|\n",
      "+-----+--------------------+\n",
      "| spam|here is a list of...|\n",
      "|  ham|Go until jurong p...|\n",
      "|  ham|Ok lar... Joking ...|\n",
      "| spam|Free entry in 2 a...|\n",
      "|  ham|U dun say so earl...|\n",
      "|  ham|Nah I don't think...|\n",
      "| spam|FreeMsg Hey there...|\n",
      "|  ham|Even my brother i...|\n",
      "|  ham|As per your reque...|\n",
      "| spam|WINNER!! As a val...|\n",
      "| spam|Had your mobile 1...|\n",
      "|  ham|I'm gonna be home...|\n",
      "| spam|SIX chances to wi...|\n",
      "| spam|URGENT! You have ...|\n",
      "|  ham|I've been searchi...|\n",
      "|  ham|I HAVE A DATE ON ...|\n",
      "| spam|XXXMobileMovieClu...|\n",
      "|  ham|Oh k...i'm watchi...|\n",
      "|  ham|Eh u remember how...|\n",
      "|  ham|Fine if thats th...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63a9d3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"text\",outputCol=\"words\")# input: 쪼개려고 가져오는 컬럼 , output: 결과값 저장\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"text\",outputCol=\"words\",pattern=\"\\\\W\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4106a31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|words                                                                                                                               |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[here, is, a, list, of, words]                                                                                                      |\n",
      "|[go, until, jurong, point,, crazy.., available, only, in, bugis, n, great, world, la, e, buffet..., cine, there, got, amore, wat...]|\n",
      "|[ok, lar..., joking, wif, u, oni...]                                                                                                |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized = tokenizer.transform(df) ## 실제 데이터프레임에 적용\n",
    "tokenized.select(\"words\").show(3,truncate=False)# 데이터프레임을 볼 때 잘리지 않고 끝까지 보고싶을 때 => truncate=False\n",
    "# 중간에 불용어도 몇개 출력함 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49f6f161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------+\n",
      "|words                                                                                                                      |\n",
      "+---------------------------------------------------------------------------------------------------------------------------+\n",
      "|[here, is, a, list, of, words]                                                                                             |\n",
      "|[go, until, jurong, point, crazy, available, only, in, bugis, n, great, world, la, e, buffet, cine, there, got, amore, wat]|\n",
      "|[ok, lar, joking, wif, u, oni]                                                                                             |\n",
      "+---------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regexTokenized = regexTokenizer.transform(df)# 불용여 전체를 제거하여 위에보다 더 깨끗하게 출력함\n",
    "regexTokenized.select(\"words\").show(3,truncate=False) # 이방법을 사용하는게 더 적절함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37519ce2",
   "metadata": {},
   "source": [
    "## Stop words removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cae18c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover # 불용어를 적용할 수 있음 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5de8d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df = regexTokenized\n",
    "# text_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc0c9aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "remover = StopWordsRemover(inputCol=\"words\",outputCol=\"cleaned\")# 불용어가 들어가면 삭제하는 변수를 생성함\n",
    "text_df = remover.transform(text_df)# text_df 데이터에서 remover를 적용하여 불용어를 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52a76ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+\n",
      "|label|                text|               words|             cleaned|\n",
      "+-----+--------------------+--------------------+--------------------+\n",
      "| spam|here is a list of...|[here, is, a, lis...|       [list, words]|\n",
      "|  ham|Go until jurong p...|[go, until, juron...|[go, jurong, poin...|\n",
      "|  ham|Ok lar... Joking ...|[ok, lar, joking,...|[ok, lar, joking,...|\n",
      "| spam|Free entry in 2 a...|[free, entry, in,...|[free, entry, 2, ...|\n",
      "|  ham|U dun say so earl...|[u, dun, say, so,...|[u, dun, say, ear...|\n",
      "|  ham|Nah I don't think...|[nah, i, don, t, ...|[nah, think, goes...|\n",
      "| spam|FreeMsg Hey there...|[freemsg, hey, th...|[freemsg, hey, da...|\n",
      "|  ham|Even my brother i...|[even, my, brothe...|[even, brother, l...|\n",
      "|  ham|As per your reque...|[as, per, your, r...|[per, request, me...|\n",
      "| spam|WINNER!! As a val...|[winner, as, a, v...|[winner, valued, ...|\n",
      "| spam|Had your mobile 1...|[had, your, mobil...|[mobile, 11, mont...|\n",
      "|  ham|I'm gonna be home...|[i, m, gonna, be,...|[m, gonna, home, ...|\n",
      "| spam|SIX chances to wi...|[six, chances, to...|[six, chances, wi...|\n",
      "| spam|URGENT! You have ...|[urgent, you, hav...|[urgent, won, 1, ...|\n",
      "|  ham|I've been searchi...|[i, ve, been, sea...|[ve, searching, r...|\n",
      "|  ham|I HAVE A DATE ON ...|[i, have, a, date...|      [date, sunday]|\n",
      "| spam|XXXMobileMovieClu...|[xxxmobilemoviecl...|[xxxmobilemoviecl...|\n",
      "|  ham|Oh k...i'm watchi...|[oh, k, i, m, wat...|[oh, k, m, watching]|\n",
      "|  ham|Eh u remember how...|[eh, u, remember,...|[eh, u, remember,...|\n",
      "|  ham|Fine if thats th...|[fine, if, that, ...|[fine, way, u, fe...|\n",
      "+-----+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_df.show() # 이제 이렇게 불용어를 제거한것으로 임베딩을 시작함 !!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccb8453",
   "metadata": {},
   "source": [
    "## n-grams\n",
    "- 텍스트 분석을 할 때 불용어를 제거한 데이터를 사용함\n",
    "- 단어 한개를 사용하는 것보다 n개의 단어를 묶어서 연속적으로 리스트에서 n개의 단어를 묶어서 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6c0c120",
   "metadata": {},
   "outputs": [],
   "source": [
    "## n-grams # 텍스트 마이닝\n",
    "from pyspark.ml.feature import NGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e8688f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|bigrams                                                                                                                                                                                                                   |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[here is, is a, a list, list of, of words]                                                                                                                                                                                |\n",
      "|[go until, until jurong, jurong point, point crazy, crazy available, available only, only in, in bugis, bugis n, n great, great world, world la, la e, e buffet, buffet cine, cine there, there got, got amore, amore wat]|\n",
      "|[ok lar, lar joking, joking wif, wif u, u oni]                                                                                                                                                                            |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bigram = NGram(n=2, inputCol=\"words\",outputCol=\"bigrams\") # n = 2: 두개씩 단어를 묶음 \n",
    "bigram_df = bigram.transform(text_df)\n",
    "bigram_df.select(\"bigrams\").show(3,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323716df",
   "metadata": {},
   "source": [
    "## BOW 인코딩\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34abf817",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF,  Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19f99f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|label|                text|               words|\n",
      "+-----+--------------------+--------------------+\n",
      "| spam|here is a list of...|[here, is, a, lis...|\n",
      "|  ham|Go until jurong p...|[go, until, juron...|\n",
      "|  ham|Ok lar... Joking ...|[ok, lar, joking,...|\n",
      "| spam|Free entry in 2 a...|[free, entry, in,...|\n",
      "|  ham|U dun say so earl...|[u, dun, say, so,...|\n",
      "|  ham|Nah I don't think...|[nah, i, don, t, ...|\n",
      "| spam|FreeMsg Hey there...|[freemsg, hey, th...|\n",
      "|  ham|Even my brother i...|[even, my, brothe...|\n",
      "|  ham|As per your reque...|[as, per, your, r...|\n",
      "| spam|WINNER!! As a val...|[winner, as, a, v...|\n",
      "| spam|Had your mobile 1...|[had, your, mobil...|\n",
      "|  ham|I'm gonna be home...|[i, m, gonna, be,...|\n",
      "| spam|SIX chances to wi...|[six, chances, to...|\n",
      "| spam|URGENT! You have ...|[urgent, you, hav...|\n",
      "|  ham|I've been searchi...|[i, ve, been, sea...|\n",
      "|  ham|I HAVE A DATE ON ...|[i, have, a, date...|\n",
      "| spam|XXXMobileMovieClu...|[xxxmobilemoviecl...|\n",
      "|  ham|Oh k...i'm watchi...|[oh, k, i, m, wat...|\n",
      "|  ham|Eh u remember how...|[eh, u, remember,...|\n",
      "|  ham|Fine if thats th...|[fine, if, that, ...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexTokenizer(inputCol=\"text\",outputCol=\"words\",pattern=\"\\\\W\")# 문장들을 토큰화함 -> 앞에서 한 작업 반복\n",
    "words_df = tokenizer.transform(df)\n",
    "words_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a21f5acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+\n",
      "|label|                text|               words|         hashfeature|\n",
      "+-----+--------------------+--------------------+--------------------+\n",
      "| spam|here is a list of...|[here, is, a, lis...|(20,[7,8,9,12,15]...|\n",
      "|  ham|Go until jurong p...|[go, until, juron...|(20,[0,3,4,7,8,10...|\n",
      "|  ham|Ok lar... Joking ...|[ok, lar, joking,...|(20,[3,5,9,10,15]...|\n",
      "| spam|Free entry in 2 a...|[free, entry, in,...|(20,[0,1,2,3,4,7,...|\n",
      "|  ham|U dun say so earl...|[u, dun, say, so,...|(20,[1,2,3,8,12,1...|\n",
      "|  ham|Nah I don't think...|[nah, i, don, t, ...|(20,[0,1,4,5,6,8,...|\n",
      "| spam|FreeMsg Hey there...|[freemsg, hey, th...|(20,[0,2,3,4,5,6,...|\n",
      "|  ham|Even my brother i...|[even, my, brothe...|(20,[3,5,6,8,9,10...|\n",
      "|  ham|As per your reque...|[as, per, your, r...|(20,[2,3,4,6,8,9,...|\n",
      "| spam|WINNER!! As a val...|[winner, as, a, v...|(20,[0,2,3,6,7,8,...|\n",
      "| spam|Had your mobile 1...|[had, your, mobil...|(20,[1,2,3,4,6,8,...|\n",
      "|  ham|I'm gonna be home...|[i, m, gonna, be,...|(20,[1,2,4,5,6,8,...|\n",
      "| spam|SIX chances to wi...|[six, chances, to...|(20,[0,1,3,4,6,7,...|\n",
      "| spam|URGENT! You have ...|[urgent, you, hav...|(20,[1,2,3,4,5,7,...|\n",
      "|  ham|I've been searchi...|[i, ve, been, sea...|(20,[0,1,2,3,4,5,...|\n",
      "|  ham|I HAVE A DATE ON ...|[i, have, a, date...|(20,[0,7,10,12,16...|\n",
      "| spam|XXXMobileMovieClu...|[xxxmobilemoviecl...|(20,[0,1,2,3,5,6,...|\n",
      "|  ham|Oh k...i'm watchi...|[oh, k, i, m, wat...|(20,[1,4,10,15,16...|\n",
      "|  ham|Eh u remember how...|[eh, u, remember,...|(20,[0,1,3,4,5,6,...|\n",
      "|  ham|Fine if thats th...|[fine, if, that, ...|(20,[0,3,4,5,7,9,...|\n",
      "+-----+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hashingTF = HashingTF(inputCol=\"words\",outputCol=\"hashfeature\",numFeatures=20) # words를 갖고 20개의 벡터로 표시 선언\n",
    "hashed_df = hashingTF.transform(words_df)# 데이터프레임에 적용 하여 새로운 데이터 프레임 생성\n",
    "hashed_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "135e9d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = IDF(inputCol=\"hashfeature\",outputCol=\"features\")\n",
    "idf_model = idf.fit(hashed_df) # 위에서 만든 값을 적용시킴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3be5e9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "| spam|(20,[7,8,9,12,15]...|\n",
      "|  ham|(20,[0,3,4,7,8,10...|\n",
      "|  ham|(20,[3,5,9,10,15]...|\n",
      "| spam|(20,[0,1,2,3,4,7,...|\n",
      "|  ham|(20,[1,2,3,8,12,1...|\n",
      "|  ham|(20,[0,1,4,5,6,8,...|\n",
      "| spam|(20,[0,2,3,4,5,6,...|\n",
      "|  ham|(20,[3,5,6,8,9,10...|\n",
      "|  ham|(20,[2,3,4,6,8,9,...|\n",
      "| spam|(20,[0,2,3,6,7,8,...|\n",
      "| spam|(20,[1,2,3,4,6,8,...|\n",
      "|  ham|(20,[1,2,4,5,6,8,...|\n",
      "| spam|(20,[0,1,3,4,6,7,...|\n",
      "| spam|(20,[1,2,3,4,5,7,...|\n",
      "|  ham|(20,[0,1,2,3,4,5,...|\n",
      "|  ham|(20,[0,7,10,12,16...|\n",
      "| spam|(20,[0,1,2,3,5,6,...|\n",
      "|  ham|(20,[1,4,10,15,16...|\n",
      "|  ham|(20,[0,1,3,4,5,6,...|\n",
      "|  ham|(20,[0,3,4,5,7,9,...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rescale = idf_model.transform(hashed_df) # 리스케일링함\n",
    "rescale.select(\"label\",\"features\").show() # 변수 2개만 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "40e4c45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0535f538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+\n",
      "|label|                text|               words|            features|\n",
      "+-----+--------------------+--------------------+--------------------+\n",
      "| spam|here is a list of...|[here, is, a, lis...|       (5,[3],[1.0])|\n",
      "|  ham|Go until jurong p...|[go, until, juron...|           (5,[],[])|\n",
      "|  ham|Ok lar... Joking ...|[ok, lar, joking,...|           (5,[],[])|\n",
      "| spam|Free entry in 2 a...|[free, entry, in,...| (5,[1,3],[3.0,1.0])|\n",
      "|  ham|U dun say so earl...|[u, dun, say, so,...|           (5,[],[])|\n",
      "|  ham|Nah I don't think...|[nah, i, don, t, ...| (5,[0,1],[1.0,1.0])|\n",
      "| spam|FreeMsg Hey there...|[freemsg, hey, th...|(5,[0,1,2],[1.0,2...|\n",
      "|  ham|Even my brother i...|[even, my, brothe...|       (5,[1],[1.0])|\n",
      "|  ham|As per your reque...|[as, per, your, r...|       (5,[1],[1.0])|\n",
      "| spam|WINNER!! As a val...|[winner, as, a, v...|(5,[1,2,3],[2.0,1...|\n",
      "| spam|Had your mobile 1...|[had, your, mobil...| (5,[1,4],[2.0,2.0])|\n",
      "|  ham|I'm gonna be home...|[i, m, gonna, be,...| (5,[0,1],[3.0,1.0])|\n",
      "| spam|SIX chances to wi...|[six, chances, to...|       (5,[1],[3.0])|\n",
      "| spam|URGENT! You have ...|[urgent, you, hav...|(5,[1,2,3,4],[1.0...|\n",
      "|  ham|I've been searchi...|[i, ve, been, sea...|(5,[0,1,2,3,4],[3...|\n",
      "|  ham|I HAVE A DATE ON ...|[i, have, a, date...| (5,[0,3],[1.0,1.0])|\n",
      "| spam|XXXMobileMovieClu...|[xxxmobilemoviecl...| (5,[1,4],[1.0,2.0])|\n",
      "|  ham|Oh k...i'm watchi...|[oh, k, i, m, wat...|       (5,[0],[1.0])|\n",
      "|  ham|Eh u remember how...|[eh, u, remember,...|       (5,[0],[2.0])|\n",
      "|  ham|Fine if thats th...|[fine, if, that, ...|       (5,[4],[2.0])|\n",
      "+-----+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(inputCol=\"words\",outputCol=\"features\",vocabSize=5, minDF=2.0)\n",
    "model = cv.fit(words_df)\n",
    "res = model.transform(words_df)\n",
    "res.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbe96a6",
   "metadata": {},
   "source": [
    "#### 수치화 된 값을 갖고 모델생성하고 분석하는 과정을 이어서 진행함"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
