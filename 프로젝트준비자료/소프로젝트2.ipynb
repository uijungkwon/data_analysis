{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## classification - 앙상블"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #1) import\n",
    "    import numpy as np  # 선형대수, 행렬, 벡터\n",
    "    import pandas as pd  # CSV파일 읽기, DataFrame 객체, 평균, 중앙값, 분산, 표준편차, 사분위수, 상관관계\n",
    "    import matplotlib.pyplot as plt  # 박스플랏, 산점도import statsmodels.formula.api as smf\n",
    "    from statsmodels.graphics.mosaicplot \n",
    "    import mosaicfrom sklearn.model_selection import train_test_splitfrom sklearn \n",
    "    import treeimport pydotplusfrom IPython.display import Imagefrom sklearn.tree \n",
    "    import export_graphvizfrom sklearn.metrics import roc_auc_scorefrom sklearn.metrics \n",
    "    import roc_curvefrom sklearn.ensemble import RandomForestClassifierfrom sklearn.metrics \n",
    "    import accuracy_scorefrom sklearn.metrics import confusion_matriximport seaborn as sns\n",
    "    \n",
    "    #2) 데이터 가져오기\n",
    "    hmeq = pd.read_csv('hmeq.txt',sep='\\t')\n",
    "    ##옵션 사용하여 데이터 가져오기\n",
    "    ##df_pd = pd.read_csv(\"projectdataO.csv\", header = None, encoding = 'cp949')\n",
    "    \n",
    "    ##데이터 탐색\n",
    "    hmeq.describe()\n",
    "    hmeq.head(10)\n",
    "    hmeq = hmeq.drop(['REASON','JOB'], axis=1)\n",
    "    \n",
    "    ##변수명 변경\n",
    "    ##df_pd.columns = ['X1','X2', 'X3','X4','X5', 'X6','X7','X8', 'X9','X10']\n",
    "    \n",
    "    ##결측치 처리\n",
    "    hmeq = hmeq.fillna(hmeq.median())\n",
    "    ###행제거\n",
    "    ##df_sp.na.drop().show()\n",
    "    ######프로젝트 1 자료 참고\n",
    "    \n",
    "    #3)데이터 분류\n",
    "    ## 설명변수 생성\n",
    "    modelfit_X = hmeq.iloc[:,1:]\n",
    "    modelfit_X.head()\n",
    "    ## 반응변수 생성\n",
    "    modelfit_y = hmeq.iloc[:,0]\n",
    "    modelfit_y.head()\n",
    "    ## 데이터 분류\n",
    "    X_train, X_test, y_train, y_test = train_test_split(modelfit_X, modelfit_y, test_size=0.4, random_state=0)\n",
    "    Xname = ['LOAN','MORTDUE','VALUE','YOJ','DEROG','DELINQ','CLAGE','NINQ','CLNO','DEBTINC']\n",
    "    yname = ['good','bad']\n",
    "    \n",
    "    #4) Decision classifier  - default tree 생성\n",
    "    cart0 = tree.DecisionTreeClassifier(criterion='gini',random_state=0) ## criterion = 'entropy'\n",
    "    \t\t###파라미터\n",
    "    \t\t### max_depth\n",
    "    \t\t### min_samles_split:노드를 split하기 위해 요구되는 최소한의 샘플 개수, 적을수록 복잡해짐\n",
    "    \t\t### min_impurity_decrease : 불순도를 감소시키도록 split, 작을수록 더 복잡해\n",
    "    \t\t### criterion : 계산하는 방법을 의미하며 그중 하나가 지니\n",
    "    cart0.fit(X_train, y_train)\n",
    "    dot_data = export_graphviz(cart0, out_file=None, feature_names=Xname,\n",
    "        class_names=yname,filled=True, rounded=True, special_characters=True)\n",
    "        ### filled: 그림에 색상을 넣을 것인가, rounded: 반올림을 진행할것인지, special_characters: 특수문자를 사용하는지\n",
    "    graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "    Image(graph.create_png())\n",
    "    \n",
    "    #4) 정확도 비교\n",
    "    y_prob0 = cart0.predict_proba(X_test)[:,1] ##예측값 출력 1번만 가져오란 의미\n",
    "    \n",
    "    #5) 앙상블 모델 - 배깅\n",
    "    bagging = RandomForestClassifier(n_estimators=100, max_features=None, random_state=1234)\n",
    "    bagging.fit(X_train, y_train)\n",
    "    ##파라미터\n",
    "    \t###n_estimators:결정트리의 갯수를 지정\n",
    "    \t###min_samples_split:노드를 분할하기 위한 최소한의 \"샘플 데이터수\", 작게 설정할수록 과적합\n",
    "    \t###min_samples_leaf: 리프노드가 되기 위해 필요한 최소한의 샘플 데이터수\n",
    "    \t\n",
    "    \t###max_features: 최적의 분할을 위해 고려할 최대 feature 개수\n",
    "    \t### Default = 'auto' (결정트리에서는 default가 none이었음)\n",
    "    \t### int형으로 지정 →피처 갯수 / float형으로 지정 →비중\n",
    "    \t### sqrt 또는 auto : 전체 피처 중 √(피처개수) 만큼 선정\n",
    "    \t### log : 전체 피처 중 log2(전체 피처 개수) 만큼 선정\n",
    "    \t###max_depth : 트리의 최대 깊이, 완벽하게 클래스 값이 결정될 때 까지 분할또는 데이터 개수가 min_samples_split보다 작아질 때까지 분할\n",
    "    \n",
    "    #5) 정확도 측정\n",
    "    bagging_pred = bagging.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, bagging_pred) ##accuracy score 사용!!\n",
    "    print(f'Mean accuracy score: {accuracy:.3}')\n",
    "    ##Confusion Matrix\n",
    "    cm = pd.DataFrame(confusion_matrix(y_test, bagging_pred), columns=yname, index=yname)\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\") ##시각화\n",
    "    ##Prediction probability\n",
    "    y_prob_bag = bagging.predict_proba(X_test)[:,1] # 맞을 확률구함\n",
    "    roc_auc_bag = roc_auc_score(y_test, y_prob_bag )\n",
    "    \n",
    "    #5-2) 앙상블 모델 - RF\n",
    "    rf = RandomForestClassifier(n_estimators=100, max_features='sqrt', oob_score=True, random_state=1234)\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    #5-2) 정확도 측정\n",
    "    ## Accuracy\n",
    "    rf_pred = rf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, rf_pred) # 정확도 \n",
    "    print(f'Mean accuracy score: {accuracy:.3}')\n",
    "    \n",
    "    ## Confusion Matrix\n",
    "    cm = pd.DataFrame(confusion_matrix(y_test, rf_pred), columns=yname, index=yname)\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    \n",
    "    y_prob_rf = rf.predict_proba(X_test)[:,1]\n",
    "    roc_auc_rf = roc_auc_score(y_test, y_prob_rf )\n",
    "    \n",
    "    fpr_rf, tpr_rf, thresholds_rf = roc_curve(y_test, y_prob_rf)\n",
    "    \n",
    "    #6) features 중요도 측정\n",
    "    importances = rf1.feature_importances_\n",
    "    \n",
    "    for name, importance in zip(Xname, importances): print(name, \"=\", importance)\n",
    "    indices = np.argsort(importances)\n",
    "    plt.title('Feature Importances')\n",
    "    plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
    "    plt.yticks(range(len(indices)),X_train.columns[indices])\n",
    "    plt.xlabel('Relative Importance')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 스팸 분석 - 파이썬 & 스파크"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords') # 업그레이드 한번만 실행\n",
    "\n",
    "#1)데이터 읽기 \n",
    "df = pd.read_table(\"SMSSpamCollection\",header=None,sep=\"\\t\")\n",
    "df = df.rename(columns={0: 'label',1: 'messages'})\n",
    "\n",
    "#2)stopWords제거\n",
    "\n",
    "## stopWords 감지하는 함수\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "def 불용어제거(텍스트):\n",
    "    텍스트 = 텍스트.lower()\n",
    "    텍스트 = re.sub(r'[^0-9a-zA-Z]', ' ', 텍스트)\n",
    "    텍스트 = re.sub(r'\\s+', ' ', 텍스트)\n",
    "    텍스트 = \" \".join(word for word in 텍스트.split() if word not in STOPWORDS)\n",
    "    return 텍스트\n",
    "\n",
    "##re모듈\n",
    "df['processed'] = df['messages'].apply(불용어제거)\n",
    "\n",
    "\n",
    "#3) 모델 트레이닝\n",
    "## 모델 생성하는 함수 생성\n",
    "X = df['processed']\n",
    "y = df['label']\n",
    "def SMS분류(model, X, y):\n",
    "\t\t#데이터 분류\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=316, shuffle=True, stratify=y) #stratify :데이터 층화 추출 => 각 계층에서 테스트 25% 트레인 75% 추출\n",
    "    # model training\n",
    "    pipeline_model = Pipeline([('vect', CountVectorizer()), # 우리가 가진 데이터에 함수를 연쇄 적용하여 최종적으로 \"model\"에 적용\n",
    "                               ('tfidf',TfidfTransformer()),\n",
    "                               ('clf', model)])\n",
    "    pipeline_model.fit(x_train, y_train)# train값을 적용시켜서 모델을 생성함     \n",
    "    print('Accuracy:', pipeline_model.score(x_test, y_test)*100)# test값을 가지고 모델의 score을 계산 (정확도)    \n",
    "    \n",
    "    y_pred = pipeline_model.predict(x_test)# 예측값 구함 \n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    cv_score = cross_val_score(pipeline_model, X, y, cv=5) #cv를 사용하여 score 구함 \n",
    "    print(\"CV Score:\", np.mean(cv_score)*100)\n",
    "\n",
    "### Logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "SMS분류(model, X, y)\n",
    "\n",
    "### Naive Bayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model = MultinomialNB()\n",
    "SMS분류(model, X, y)\n",
    "\n",
    "### Support Vector Machine\n",
    "from sklearn.svm import SVC\n",
    "model = SVC(C=3)\n",
    "SMS분류(model, X, y)\n",
    "\n",
    "### Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier()\n",
    "SMS분류(model, X, y)\n",
    "\n",
    "##예측값을 구하는 함수 \n",
    "def SMS예측(model, x_train, y_train, x_test):\n",
    "    pipeline_model = Pipeline([('vect', CountVectorizer()),\n",
    "                               ('tfidf',TfidfTransformer()),\n",
    "                               ('clf', model)])\n",
    "    pipeline_model.fit(x_train, y_train)    \n",
    "    y_pred = pipeline_model.predict(x_test)\n",
    "    return y_pred\n",
    "\n",
    "##데이터 분할\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=316, shuffle=True, stratify=y)\n",
    "##모델 생성\n",
    "model = SVC(C=3)\n",
    "##예측값 함 \n",
    "예측label = SMS예측(model, x_train, y_train, x_test)\n",
    "print(type(y_test),type(예측label))\n",
    "print(list(y_test[:10])) #리스트 형태로 변환하여 같이봄 (데이터 형태가 다르니까)\n",
    "print(예측label[:10])\n",
    "\n",
    "##정확도 출력\n",
    "from sklearn.metrics import confusion_matrix\n",
    "혼동행렬 = confusion_matrix(y_test, 예측label) # 테스트값과 예측값 비교하여 정확도 출력\n",
    "print(혼동행렬)\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "print(\"Precision:\",precision_score(y_test, 예측label, pos_label=\"ham\"))\n",
    "print(\"Recall:\",recall_score(y_test, 예측label, pos_label=\"ham\"))\n",
    "print(\"F1:\",f1_score(y_test, 예측label, pos_label=\"ham\"))\n",
    "\n",
    "#1) countVector 사용\n",
    "예제문장 =  ['This is the first document.', 'This document is the second document.', \n",
    "         'And this is the third one.', 'Is this the first document?']\n",
    "## 토큰 생성 \n",
    "vectorizer = CountVectorizer()\n",
    "토큰개수 = vectorizer.fit_transform(예제문장)\n",
    "vectorizer.get_feature_names_out() # 토큰들을 array로 출력\n",
    "\n",
    "print(vectorizer.vocabulary_) # 각 토큰의 개수를 나타냄 0,1\n",
    "\n",
    "## 단어의 가중치를 조정한 벡터를 생성\n",
    "vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2)) # 2개씩 데이터를 연달아 묶어서 표현-> 알파벳순으로 출력\n",
    "토큰개수2 =  vectorizer2.fit_transform(예제문장)\n",
    "vectorizer2.get_feature_names_out()\n",
    "\n",
    "##\n",
    "단어장 = ['this', 'document', 'first', 'is', 'second', 'the', 'and', 'one'] # 여기에 있는 단어들만 갖고 단어 빈도수를 계산하고 싶은 경우\n",
    "vectorizer3 = CountVectorizer(vocabulary=단어장)\n",
    "토큰개수3 =  vectorizer3.fit_transform(예제문장)\n",
    "vectorizer3.get_feature_names_out() #토큰 출력 \n",
    "print(토큰개수3.toarray()) # 토큰 빈도수 출력 (array형태로) 0,1 형태\n",
    "\n",
    "## 불용어 제거\n",
    "tfidf = TfidfVectorizer(stop_words = 'english').fit(예제문장)\n",
    "print(tfidf.vocabulary_) # 각 토큰의 개수를 나타냄\n",
    "print(tfidf.transform(예제문장).toarray()) # 가중치를 고려하여 해당 데이터의 비중을 표현 -> 가중치 값을 표현 \n",
    "# -> 이 값을 비교하여 값이 거의 비슷한 데이터를 추출할 수 있음\n",
    "\n",
    "##파이프라인 사용\n",
    "파이프 = Pipeline([('count', CountVectorizer(vocabulary=단어장)), \n",
    "                 ('tfid', TfidfTransformer())]).fit(예제문장)\n",
    "토큰개수  = 파이프['count'].transform(예제문장)\n",
    "파이프['tfid'].idf_\n",
    "파이프.transform(예제문장).toarray()\n",
    "\n",
    "#############################################\n",
    "#############################################\n",
    "#PYSPARK#\n",
    "\n",
    "#1) import문\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "countTokenizer = udf(lambda w: len(w), IntegerType())\n",
    "##불용어\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "##TF-IDF\n",
    "from pyspark.ml.feature import IDF\n",
    "\n",
    "#2) 데이터 읽기 및 변수명 변경\n",
    "spark = SparkSession.builder.appName(\"nlp_nb\").getOrCreate()\n",
    "df = spark.read.csv(\"SMSSpamCollection\",inferSchema=True,sep=\"\\t\")\n",
    "df = df.withColumnRenamed(\"_c0\",\"label\").withColumnRenamed(\"_c1\",\"messages\")\n",
    "\n",
    "df = df.withColumn(\"length\",length(df[\"messages\"]))\n",
    "df.groupby(\"label\").mean().show()\n",
    "\n",
    "#3) 토큰화\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "##\n",
    "tokenizer = Tokenizer(inputCol=\"messages\",outputCol=\"tokened\")\n",
    "stop_word_remover = StopWordsRemover(inputCol=\"tokened\",outputCol=\"stoped\")\n",
    "count_vec = CountVectorizer(inputCol=\"stoped\",outputCol=\"c_vec\")\n",
    "idf = IDF(inputCol=\"c_vec\",outputCol=\"tf_idf\")\n",
    "ham_spam_to_num = StringIndexer(inputCol=\"label\",outputCol=\"label_01\")\n",
    "##\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vector\n",
    "cleaned = VectorAssembler(inputCols=['tf_idf','length'],outputCol=\"features\")\n",
    "\n",
    "##파이프라인\n",
    "from pyspark.ml import Pipeline\n",
    "파이프라인 = Pipeline(stages=[ham_spam_to_num, tokenizer,stop_word_remover,count_vec,idf,cleaned])\n",
    "전처리기 = 파이프라인.fit(df)\n",
    "전처리_df = 전처리기.transform(df)\n",
    "전처리_df.show()\n",
    "\n",
    "#3) 모델 생성 및 평가\n",
    "최종_df = 전처리_df.select(['label_01','features']).withColumnRenamed(\"label_01\",\"label\")\n",
    "(\n",
    "train_df, test_df) = 최종_df.randomSplit([.75,.25], seed= 316)\n",
    "\n",
    "### Naive Bayes Model\n",
    "from pyspark.ml.classification import NaiveBayes, LogisticRegression, RandomForestClassifier, GBTClassifier, LinearSVC\n",
    "nb = NaiveBayes()\n",
    "적합모형 = nb.fit(train_df)\n",
    "적합결과 = 적합모형.transform(test_df)\n",
    "\n",
    "### MulticlassClassificationEvaluator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "eval = MulticlassClassificationEvaluator()\n",
    "acc = eval.evaluate(적합결과)\n",
    "print(f\"Accuracy:{acc*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear & Logistic Regression -파이썬, 스파크\n",
    "### 파이썬: Sklearn VS statsmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1) linear regression - 사이킷 런 VS Statsmodel!\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression  \n",
    "import statsmodels.api as sm\n",
    "##데이터 가져오기\n",
    "whitedata = pd.read_csv(\"white.csv\")\n",
    "##반응 변수 설명변수 설정\n",
    "y = whitedata.y  # whitedata[\"y\"] 같은의미\n",
    "x = whitedata.x1\n",
    "plt.scatter(x,y,color=\"blue\"); plt.show()# 산점도 그리기\n",
    "##반응변수, 설명변수 설정2\n",
    "x = whitedata[\"x1\"].to_frame() #시리즈를 데이터프레임으로 변환\n",
    "y = whitedata.y\n",
    "## linearRegression 모델 생성\n",
    "slm = LinearRegression().fit(x,y)       \n",
    "w, b = slm.coef_[0], slm.intercept_ #coef = B1(추정된 가중치 벡터), intercept = B0 회귀계수 (추정된 상수항)\n",
    "## 모델 시각화\n",
    "plt.scatter(x,y,color=\"blue\")\n",
    "plt.plot([x0,x1],[w*x0+b,w*x1+b],'r'); plt.show()# 기울기 선 추가\n",
    "\n",
    "#2) linear regression - statsmodels\n",
    "## statsmodel의 사용\n",
    "model = sm.OLS(y, sm.add_constant(x))   #OLS: 잔차제곱합을 최소화하는 가중치 벡터를 구하는 방법\n",
    "results = model.fit()   # sm.add_constant(x) : 상수항 결합, 선형회귀모형에 반드시 필요\n",
    "print(results.summary()) #OLS Regression 결과표를 출력\n",
    "\n",
    "#2) 다중선형 회귀 모형 생성\n",
    "## 반응변수 설명변수 생성\n",
    "X = whitedata.drop([\"y\"],axis=1)\n",
    "y = whitedata[\"y\"]\n",
    "##데이터 분리\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=100)\n",
    "##regression 모델 생성\n",
    "lrm = LinearRegression(n_jobs=-1)      # fit_intercept=True  # -1: 지금있는 모든 core를 모두 사용하라는 뜻\n",
    "result = lrm.fit(X_train, y_train) \n",
    "print(result.coef_) #상관계수 출력 \n",
    "##테스트 값으로 예측값 생성\n",
    "forecast = lrm.predict(X_test) #새로운 입력 데이터에 대한 출력데이터 예측\n",
    "##테스트값과 예측값 비교하여 정확도 분석 \n",
    "accuracy = lrm.score(X_test, y_test)\n",
    "\n",
    "## OLS regression -statsmodel \n",
    "model = sm.OLS(y_train, sm.add_constant(X_train)) \n",
    "results = model.fit()  \n",
    "print(results.summary())# 결과표 출력 \n",
    "\n",
    "#3) LASSO\n",
    "## 모델 생성\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "lasso = Lasso(alpha=0.1)\n",
    "ridge = Ridge(alpha=0.1)\n",
    "##모델 적합\n",
    "lasso.fit(X_train, y_train)\n",
    "print(lasso.intercept_ , lasso.coef_ )\n",
    "##예측값 생성\n",
    "forecast = lasso.predict(X_test) # 테스트 데이터로 예측함\n",
    "print(forecast[0:10])\n",
    "\n",
    "## training-validation procedure(반복 with train-test-split) # 무작위로 10번 test # 설명변수가 많은경우 모델을 fitting하면서 설명변수를 선택할 때 사용, 약간의 bias가 발생할 수 있다.\n",
    "from sklearn.metrics import mean_squared_error\n",
    "result = np.zeros((10,5))\n",
    "a = [0.001, 0.002, 0.003, 0.004, 0.005] # \"유의수준\" 지정 -> 0.001을 선택하는 것이 좋음 \n",
    "for step in range(10):\n",
    "    X0, X1, y0, y1 = train_test_split(X, y, test_size=0.3)\n",
    "    for choice in range(len(a)):\n",
    "        lasso = Lasso(alpha=a[choice])      # sklearn.preprocessing.StandardScaler\n",
    "        lasso.fit(X0, y0); forecast = lasso.predict(X1)\n",
    "        result[step,choice] = mean_squared_error(y1,forecast)\n",
    "print(result)\n",
    "result = pd.DataFrame(result)\n",
    "result.mean()\n",
    "\n",
    "#4) Logistic Regression - statsmodel\n",
    "##데이터 분류\n",
    "whitedata[\"good\"] = (whitedata.y > 5).astype(float) #good 칼럼의 데이터만 가져옴\n",
    "y_binary = whitedata[\"good\"] #target value\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.3, random_state=100)\n",
    "##모델 생성\n",
    "logit_sm = sm.Logit(y_train, sm.add_constant(X_train)).fit()\n",
    "print(logit_sm.summary())\n",
    "\n",
    "print(\"Parameters:\",logit_sm.params)# sm모델의 파라미터 출력\n",
    "print(\"Eta:\", logit_sm.fittedvalues[0:10]\n",
    "## 예측값 생성\n",
    "muhat = logit_sm.predict(sm.add_constant(X_test)) #새로운 데이터로 타겟값을 예측 \n",
    "print(\"prob:\", muhat[0:10])\n",
    "yhat = (muhat > 0.5).astype(int) # true =1, false=0 # y예측값을 구함\n",
    "print(yhat[0:10])\n",
    "##정확도 측정\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "cm = confusion_matrix(y_test, yhat) #정확도 매트릭스를 사용하여 y의 테스트값과, 예측값인 yhat 값 비교\n",
    "print (\"Confusion Matrix : \\n\", cm) \n",
    "##TP FN   (실제 x 예측)\n",
    "##FP TN\n",
    "##accuracy score of the model\n",
    "print('Test accuracy = ', accuracy_score(y_test, yhat))\n",
    "      \n",
    "##statsmodel - logit모델 생성\n",
    "import statsmodels.formula.api as smf\n",
    "model = \"good ~ x2+x4+x6+x10+x11\" #칼럼 X1~X11을 사용해여 모델 생성\n",
    "logit_smf = smf.logit(formula=str(model),data=whitedata).fit()\n",
    "logit_smf.summary() ##logistic regression의 결과를 표로 출력\n",
    "\n",
    "##sklearn linear model 생성\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logitS = LogisticRegression(penalty=None)  ## C=1/alpha=1, penality=\"l2\" {\"l1\",\"l2\",\"elasticnet\",None}\n",
    "logitS.fit(X_train, y_train)\n",
    "print(logitS.coef_, logitS.intercept_)\n",
    "logitL = LogisticRegression(penalty=\"l1\",solver=\"saga\")  # LASSO , C = 1/alpha, solver를 saga로 적용 후 사용가능함, 라쏘는 기준값 이하로 내려가면 0으로 보내버림\n",
    "logitL.fit(X_train, y_train)\n",
    "##정확도 예측\n",
    "muL = logitL.predict_proba(X_test) # 뮤 햇을 구하여 muL 생성 (예측 확률 구함)\n",
    "yhat = logitL.predict(X_test)  # 예측값을 생성 \n",
    "print(muL[0:10])\n",
    "print(yhat[0:10]) #결과값은 0 ,1 로 표현\n",
    "##예측값과 정답을 비교하여 정확도 출력 \n",
    "logitL.score(X_test, y_test)\n",
    "      \n",
    "##########################################\n",
    "##########################################\n",
    "##SPARK##\n",
    "##regression\n",
    "\n",
    "\n",
    "#1) 스파크 생성\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark =  SparkSession.builder.appName(\"wine\").getOrCreate()\n",
    "white  = spark.read.csv(\"white.csv\",inferSchema=True,header=True)\n",
    "\n",
    "#2) 컬럼명으로 구성된 list 생성 \n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "설명변수 = list(white.columns)# 칼럼명만 들어있는 리스트 \n",
    "\n",
    "변수묶음 = VectorAssembler(inputCols=설명변수,outputCol=\"features\")\n",
    "변환자료  = 변수묶음.transform(white) #각 칼럼의 값이 list형태로 행기준으로 묶임\n",
    "\n",
    "#3)Linear Regression\n",
    "##회귀자료 생성\n",
    "회귀자료 = 변환자료.select([\"features\",\"y\"])#설명변수, 반응변수\n",
    "##자료 분할\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "train_data, test_data = 회귀자료.randomSplit([0.7, 0.3], 316)\n",
    "##모델 생성\n",
    "회귀분석  = LinearRegression(featuresCol=\"features\",labelCol=\"y\").fit(train_data)\n",
    "회귀분석.coefficients # 상관계수\n",
    "회귀분석.intercept #회귀계수 (상수함)\n",
    "pred = 회귀분석.evaluate(test_data) \n",
    "pred.predictions.show() #테스트자료로 예측값(반응변수) 출력 \n",
    "# 결정계수, MSE(모분산 추정)\n",
    "print(pred.r2, pred.meanSquaredError)\n",
    "\n",
    "######### PYSPARK logistic 모델 생성\n",
    "\n",
    "\n",
    "##문자변수만 모아 생성\n",
    "from pyspark.sql.types import StringType\n",
    "문자변수 = [변수.name for 변수 in churn_df.schema.fields if isinstance(변수.dataType, StringType)] #데이터프레임의 범주의 타입이 문자인지 확인하고 맞다면 출력\n",
    "문자변수 # 앞으로 이 문자변수를 갖고 숫자 변수로 변화해주는 작업을 할것임\n",
    "\n",
    "#1) 문자변수를 숫자값을 가진 변수로 변환\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "##-1 StringIndexer\n",
    "indexer  = StringIndexer(inputCols=문자변수, #내가 바꿀 변수(1개 이상의 값을 가진 경우)\n",
    "                         outputCols=[\"{}_SI\".format(c) for c in 문자변수]) # 컬럼명을 변경\n",
    "encode_df  = indexer.fit(churn_df).transform(churn_df) #기존 데이터프레임에 새로운 변수값들이 추가 -> 타입이 숫자로 변환\n",
    "encode_df.printSchema()\n",
    "\n",
    "##-2 OneHotEncoder : 숫자 변경\n",
    "oneHot = OneHotEncoder(inputCols = [\"{}_SI\".format(c) for c in 문자변수],\n",
    "                        outputCols =[\"{}_encoder\".format(c) for c in 문자변수], dropLast =False )\n",
    "oneHot_df = oneHot.fit(encode_df).transform(encode_df)\n",
    "oneHot_df.limit(5).show()\n",
    "##숫자로 구성된 칼럼 생성\n",
    "설명변수 = [\"SeniorCitizen\",\"MonthlyCharges\"]+[\"{}_SI\".format(c) for c in 문자변수] #기존의 숫자칼럼 + 내가 변환한 숫자칼럼 => 하나의 데이터프레임으로 형성\n",
    "설명변수 = 설명변수[0:-1]\n",
    "\n",
    "#2) features 생성 -> 각 칼럼의 값들을 행 기준으로 리스트로 묶\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "변수묶음 = VectorAssembler(inputCols=설명변수,outputCol=\"features\") #output은 하나의 묶여진 형태로 나옴\n",
    "변환자료  = 변수묶음.transform(encode_df)\n",
    "변환자료.select(\"features\",\"Churn_SI\").show()\n",
    "분류자료 = 변환자료.select([\"features\",\"Churn_SI\"])\n",
    "\n",
    "\n",
    "#3)logistic regression model 생성\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "##데이터 분류\n",
    "train_data, test_data =분류자료.randomSplit([0.7, 0.3], 316)\n",
    "## 모델 생성 \n",
    "분석모형 =  LogisticRegression(labelCol=\"Churn_SI\").fit(train_data) # 반응변수:labelCol\n",
    "분석모형.summary\n",
    "##반응변수의 예측값 출력\n",
    "분석모형.summary.predictions.show(truncate=False)\n",
    "##기술 통계값 계산\n",
    "분석모형.summary.predictions.describe().show()\n",
    "\n",
    "#4) 모델 평가\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "##테스트 데이터로 평\n",
    "예측 = 분석모형.evaluate(test_data) # 테스트 데이터로 모델을 평가해야함\n",
    "예측.predictions.show()\n",
    "##함수를 사용하여 정확도 평\n",
    "평가 = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\",labelCol=\"Churn_SI\") ## labelCol = 타겟값\n",
    "auc = 평가.evaluate(예측.predictions) # 예측값 출력\n",
    "auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification -sklearn\n",
    "### 자세한 방식보다 전체적으로 모델 어떻게 생성하는지 설명"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1) 데이터 가져오기\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn import tree\n",
    "iris = load_iris()\n",
    "\n",
    "#2) 모델 생성\n",
    "clf = tree.DecisionTreeClassifier().fit(iris.data, iris.target) #(설명변수, 반응변수)\n",
    "\n",
    "###Graphviz 설치\n",
    "\n",
    "#3)decision tree 사진을 생성\n",
    "import pydotplus\n",
    "from IPython.display import Image\n",
    "dot_data = tree.export_graphviz(clf, out_file=None,\n",
    "                                 feature_names=iris.feature_names,\n",
    "                                 class_names=iris.target_names,\n",
    "                                filled=True, rounded=True,\n",
    "                                special_characters=True)\n",
    "\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "graph.write_pdf(\"iris.pdf\") ##pdf로 저장\n",
    "Image(graph.create_png())\n",
    "\n",
    "#4) 앙상블. 모형 소개 -> 배깅, 부스팅, 랜덤포레스트\n",
    "##모델. 생성을 위한 임포트\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import graphviz#from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.ensemble import BaggingClassifier, GradientBoostingClassifier, RandomForestClassifier ##sklearn에서 사용!\n",
    "from sklearn.metrics import confusion_matrix, mean_squared_error\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "## 데이터 분할 from sklearn.model_selection import train_test_split\n",
    "iris = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=100)\n",
    "\n",
    "#4-1) Baggin Classifier 생성\n",
    "m_bagging = BaggingClassifier()\n",
    "m_bagging.fit(X_train,y_train)\n",
    "\n",
    "##테스트값 예측\n",
    "pred_bagging = m_bagging.predict(X_test)\n",
    "## 실제 y값과 테스트값 비교(confusion matrix 사용)\n",
    "cf_bagging=confusion_matrix(y_test,pred_bagging)\n",
    "print('Bagging Confusion Matrix')\n",
    "print(cf_bagging)\n",
    "\n",
    "## 실제 y값과 테스트값 비교-> 정확도 출력(score)\n",
    "score_bagging=m_bagging.score(X_test, y_test)\n",
    "print('Bagging Accuracy')\n",
    "print(score_bagging)\n",
    "\n",
    "#4-2)boosting\n",
    "m_boosting = GradientBoostingClassifier()\n",
    "m_boosting.fit(X_train,y_train)\n",
    "## 테스트 값으로 label예측\n",
    "pred_boosting = m_boosting.predict(X_test)\n",
    "## confusion matrix로 정확도 예측\n",
    "cf_boosting = confusion_matrix(y_test,pred_boosting)\n",
    "print('Boosting Confusion Matrix')\n",
    "print(cf_boosting)\n",
    "## 테스트 값과 실제 y값을 비교하여 score 생성\n",
    "score_boosting = m_boosting.score(X_test, y_test)\n",
    "print('Boosting Accuracy'\n",
    ")print(score_boosting)\n",
    "\n",
    "#4-3) RF\n",
    "m_RF = RandomForestClassifier(n_estimators=1000) ##1000번 반복함\n",
    "m_RF.fit(X_train,y_train)\n",
    "##테스트 값으로 label 예측\n",
    "pred_RF = m_RF.predict(X_test)\n",
    "##confusion matrix 사용하여 테스트값과 y값 비교\n",
    "cf_RF = confusion_matrix(y_test,pred_RF)\n",
    "print('Random Forest Confusion Matrix')\n",
    "print(cf_RF)\n",
    "## 정확도 출력\n",
    "score_RF = m_RF.score(X_test, y_test)\n",
    "print('Random Forest Accuracy')print(score_RF)\n",
    "## 모델링에서 많이 언급된 변수를 추출\n",
    "###세번째 변수가 가장 많이 언급\n",
    "importances = m_RF.feature_importances_\n",
    "importances\n",
    "##변동성 측정 \n",
    "std = np.std([m_RF.feature_importances_ for tree in m_RF.estimators_], axis=0)\n",
    "변수명 = [f\"feature {i}\" for i in range(iris.data.shape[1])]\n",
    "변수명\n",
    "## 사용된 변수 importance 시각화 \n",
    "forest_importances = pd.Series(importances, index=변수명)\n",
    "fig, ax = plt.subplots()\n",
    "forest_importances.plot.bar(yerr=std, ax=ax)\n",
    "ax.set_title(\"Feature importances using MDI\")\n",
    "ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering - kmeans Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "iris = load_iris() ## array형태로 저장되어있음\n",
    "\n",
    "iris_df = pd.DataFrame(data=iris.data, columns=[\"sepal_length\",\"sepal_width\",\"petal_length\",\"petal_width\"]) # 컬럼 이름 지정\n",
    "iris_df.head()\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, max_iter=300, random_state=316) #3개의 클러스털 생성, 316번동안 모델링함\n",
    "kmeans.fit(iris_df)# 모델생성 후 fit 시킴\n",
    "\n",
    "print(kmeans.labels_) # 예측한값  라벨 확인\n",
    "\n",
    "iris_df[\"target\"] = iris.target # 데이터에 저장되어있던 정답값\n",
    "iris_df[\"cluster\"] = kmeans.labels_\n",
    "iris_result = iris_df.groupby([\"target\",\"cluster\"])[\"sepal_length\"].count() #target: 정답값, cluster:예측값 비교\n",
    "print(iris_result)\n",
    "\n",
    "## 군집화 평가 - 실루엣 계수 사용\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "iris_df[\"silhouette\"] = silhouette_samples(iris.data, iris_df[\"cluster\"])\n",
    "\n",
    "ave_score = silhouette_score(iris.data,iris_df[\"cluster\"])\n",
    "print(\"SAS:{0:.4f}\".format(ave_score))\n",
    "iris_df.head()\n",
    "\n",
    "iris_df.groupby(\"cluster\")[\"silhouette\"].mean() # 그룹0은 잘 분류되었지만. 1과 2는 다소 잘 분류\n",
    " \n",
    "from sklearn.mixture import GaussianMixture\n",
    "gmm = GaussianMixture(n_components=3, random_state=316).fit(iris.data)\n",
    "iris_df[\"gmm\"] = gmm.predict(iris.data)\n",
    "iris_df.head()\n",
    "\n",
    "iris_result = iris_df.groupby([\"target\"])[\"gmm\"].value_counts()\n",
    "print(iris_result) ## target: 정답, gmm: 예측값 (그룹 명이 변경될 수도 있음) , 여기 아래 예시에선 1과 0클러스터를 이름을 바꿔서 분류했다고 볼 수 있음 -> 정확도 높음 \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## classification - SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "%matplotlib inline\n",
    "\n",
    "#1) 데이터 불러오기\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "## 데이터 추출\n",
    "x = iris.data[:, [0, 2]] \n",
    "y = iris.target\n",
    "##학습자료와 검증자료 분리\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test= train_test_split(x, y, test_size=0.3, random_state=1, stratify=y)\n",
    "##데이터 표준화\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc= StandardScaler()\n",
    "sc.fit(x_train)\n",
    "###numpy.ndarray 형식으로 출력\n",
    "x_train_std = sc.transform(x_train)\n",
    "x_test_std = sc.transform(x_test)\n",
    "###x_train_std의평균과 표준편차 - array 형태\n",
    "print('Mean of x_train_std:',np.mean(x_train_std[:,0]), np.mean(x_train_std[:,1]))\n",
    "print('Stdof x_train_std:',np.std(x_train_std[:,0]), np.std(x_train_std[:,1]))\n",
    "\n",
    "#2) SVM모델 생성\n",
    "## C가작은경우(오분류를관대하게허용)\n",
    "\n",
    "## 파라미터: SVC 클래스에 kernel='linear', 시드넘버는1, C=0.1로 설정\n",
    "### C값이 달라지면 정확도가 달라짐, C가 커졌을 때 정확도 높아짐\n",
    "### gamma =1, (작은 값) = 100 : 오버피팅 발생 !!!\n",
    "from sklearn.svm import SVC\n",
    "svm_smallc= SVC(kernel='linear', random_state=1, C=0.1) ## SVC: classification ,SVR: regression\n",
    "svm_smallc.fit(x_train_std, y_train)\n",
    "##표준화된 것과 안된 것\n",
    "x_combined_std= np.vstack((x_train_std, x_test_std))\n",
    "y_combined= np.hstack((y_train, y_test))\n",
    "\n",
    "#3) 데이터와 결정 경계그림 그리기\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "plot_decision_regions(x_combined_std, y_combined, clf=svm_smallc)\n",
    "plt.xlabel('petal length [standardized]') #x축\n",
    "plt.ylabel('petal width [standardized]')  #y축\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "#4) 예측값과 관측값 비교\n",
    "y_pred= svm_smallc.predict(x_test_std)\n",
    "y_pred,(y_test!= y_pred)\n",
    "print('Misclassified samples: %d' % (y_test!= y_pred).sum()) #10개의 샘플이 잘못 분류되었다고 출력\n",
    "\n",
    "#5) 정확도 계산\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy: %.2f' % accuracy_score(y_test, y_pred)) # 정답과 추정값 비교\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
